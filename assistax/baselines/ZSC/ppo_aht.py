"""
IPPO Ad Hoc Teamwork (AHT) Training for Zero-Shot Generalization

This module implements Ad Hoc Teamwork training using IPPO, where agents learn to collaborate
effectively with diverse, pre-trained teammates from a zoo. The goal is to train agents that
can generalize to work with previously unseen partners in zero-shot scenarios.
Usage:
    python ippo_aht.py [hydra options]
    
The script requires a pre-populated zoo (generated by ippo_zoo_gen.py) containing
diverse agents from multiple algorithms.
"""

import os
import sys
import time
from tqdm import tqdm
import jax
import jax.numpy as jnp
import flax.linen as nn
from flax.linen.initializers import constant, orthogonal
from flax.training.train_state import TrainState
from flax.traverse_util import flatten_dict
import safetensors.flax
import optax
import distrax
import assistax
from assistax.wrappers.baselines import  get_space_dim, LogEnvState, LogWrapper
from assistax.wrappers.aht import ZooManager, LoadAgentWrapper
import hydra
from omegaconf import OmegaConf
import pandas as pd
from typing import Sequence, NamedTuple, Any, Dict
from assistax.baselines.utils import (
    _tree_take, _unstack_tree, _take_episode,
    _tree_shape, _stack_tree, _concat_tree, _tree_split
    )
from assistax.baselines.utils import _compute_episode_returns_sweep as _compute_episode_returns

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


# ================================ MAIN AHT TRAINING FUNCTION ================================

@hydra.main(version_base=None, config_path="config", config_name="ppo_aht")
def main(config):
    """
    Main function for Ad Hoc Teamwork training using IPPO with zoo-based partners.
    
    This function implements a sophisticated training pipeline for zero-shot generalization:
    1. Loads diverse pre-trained partners from zoo across multiple algorithms
    2. Creates train/test splits for proper generalization evaluation
    3. Trains new agents against the training set of diverse partners
    4. Evaluates on both train and test sets to measure generalization capability
    5. Saves comprehensive results for analysis of AHT performance
    
    The resulting agents should be able to collaborate effectively with previously
    unseen teammates, demonstrating robust teamwork and adaptability.
    
    Args:
        config: Hydra configuration object containing AHT training parameters,
               partner algorithms, and zoo specifications
    """
    config = OmegaConf.to_container(config, resolve=True)

    # ===== DYNAMIC ALGORITHM SELECTION =====
    # Import the appropriate IPPO variant based on network architecture configuration
    match (config["network"]["recurrent"], config["network"]["agent_param_sharing"]):
        case (False, False):
            from IPPO.ippo_ff_nps import make_train, make_evaluation, EvalInfoLogConfig
            print("Using: Feedforward Networks with No Parameter Sharing")
        case (False, True):
            from IPPO.ippo_ff_ps import make_train, make_evaluation, EvalInfoLogConfig
            print("Using: Feedforward Networks with Parameter Sharing")
        case (True, False):
            from IPPO.ippo_rnn_nps import make_train, make_evaluation, EvalInfoLogConfig
            print("Using: Recurrent Networks with No Parameter Sharing")
        case (True, True):
            from IPPO.ippo_rnn_ps import make_train, make_evaluation, EvalInfoLogConfig
            print("Using: Recurrent Networks with Parameter Sharing")

    # ===== TRAINING SETUP =====
    rng = jax.random.PRNGKey(config["SEED"])
    train_rng, eval_rng = jax.random.split(rng)
    train_rngs = jax.random.split(train_rng, config["NUM_SEEDS"])
    
    print(f"Starting Ad Hoc Teamwork training with {config['TOTAL_TIMESTEPS']} timesteps")
    print(f"Num environments: {config['NUM_ENVS']}")
    print(f"Num seeds: {config['NUM_SEEDS']}")
    print(f"Environment: {config['ENV_NAME']}")
    print(f"Zoo path: {config['ZOO_PATH']}")
    print(f"Training algorithm: {config['ALGORITHM']}")
    print(f"Partner algorithms: {config['PARTNER_ALGORITHMS']}")
    
    # ===== ZOO MANAGEMENT AND PARTNER SELECTION =====
    print("Loading zoo and selecting diverse partners...")
    with jax.disable_jit(config["DISABLE_JIT"]):
        zoo = ZooManager(config["ZOO_PATH"])
        alg = config["ALGORITHM"]
        scenario = config["ENV_NAME"]
        
        # Load partners from multiple algorithms for diversity
        partner_dict = {}
        for partner_algo in config["PARTNER_ALGORITHMS"]:
            partner_dict[partner_algo] = zoo.index.query(f'algorithm == "{partner_algo}"'
                                                         ).query(f'scenario == "{scenario}"'
                                                            ).query('scenario_agent_id == "human"')
            print(f"Found {len(partner_dict[partner_algo])} {partner_algo} partners")
        
        # ===== TRAIN/TEST SPLIT FOR GENERALIZATION EVALUATION =====
        # Critical for measuring zero-shot generalization capability
        print("Creating train/test splits for generalization evaluation...")
        all_partners = pd.concat(partner_dict.values(), ignore_index=True)
        print(f"Total partners across all algorithms: {len(all_partners)}")
   
        # Do a single 50/50 split across all partners
        train_partners = all_partners.sample(frac=0.5, random_state=42)  # Set random_state for reproducibility
        test_partners = all_partners.drop(train_partners.index)

        # Split back into algorithm-specific dictionaries
        train_set = {}
        test_set = {}

        for algo in partner_dict.keys():
            train_set[algo] = train_partners[train_partners['algorithm'] == algo].reset_index(drop=True)
            test_set[algo] = test_partners[test_partners['algorithm'] == algo].reset_index(drop=True)
            print(f" {algo}: {len(train_set[algo])} train, {len(test_set[algo])} test partners")

        # Create zoo loading dictionaries for training and testing
        load_zoo_dict_train = {algo: {"human": list(train_set[algo].agent_uuid)} for algo in partner_dict.keys()}
        load_zoo_dict_test = {algo: {"human": list(test_set[algo].agent_uuid)} for algo in partner_dict.keys()}

        print(f"Training against {sum(len(train_set[algo]) for algo in train_set)} diverse partners")
        print(f"Testing against {sum(len(test_set[algo]) for algo in test_set)} unseen partners")

        # ===== AHT TRAINING EXECUTION =====
        print("Starting Ad Hoc Teamwork training...")
        env = assistax.make(config["ENV_NAME"], **config["ENV_KWARGS"])
        
        # Create training function with zoo partner loading
        train_jit = jax.jit(
            make_train(
                config,
                save_train_state=True,
                load_zoo=load_zoo_dict_train  # Train against diverse partner set
            ),
            device=jax.devices()[config["DEVICE"]]
        )
        
        # Execute AHT training
        out = jax.vmap(train_jit, in_axes=(0, None, None, None))(
            train_rngs,
            config["LR"], config["ENT_COEF"], config["CLIP_EPS"]
        )

        # ===== SAVE TRAINING RESULTS =====
        print("Saving training results...")
        
        # Save training metrics (excluding large training states)
        EXCLUDED_METRICS = ["train_state"]
        jnp.save("metrics.npy", {
            key: val
            for key, val in out["metrics"].items()
            if key not in EXCLUDED_METRICS
            },
            allow_pickle=True
        )

        # Save model parameters
        all_train_states = out["metrics"]["train_state"]
        final_train_state = out["runner_state"].train_state
        
        safetensors.flax.save_file(
            flatten_dict(all_train_states.params, sep='/'),
            "all_params.safetensors"
        )
        
        if config["network"]["agent_param_sharing"]:
            # For parameter sharing: single set of shared parameters
            safetensors.flax.save_file(
                flatten_dict(final_train_state.params, sep='/'),
                "final_params.safetensors"
            )
        else:
            # For independent parameters: split by agent
            split_params = _unstack_tree(
                jax.tree.map(lambda x: x.swapaxes(0, 1), final_train_state.params)
            )
            for agent, params in zip(env.agents, split_params):
                safetensors.flax.save_file(
                    flatten_dict(params, sep='/'),
                    f"{agent}.safetensors",
                )

        # ===== GENERALIZATION EVALUATION SETUP =====
        print("Setting up generalization evaluation...")
        
        # Calculate evaluation batching for memory efficiency
        batch_dims = jax.tree.leaves(_tree_shape(all_train_states.params))[:2]
        n_sequential_evals = int(jnp.ceil(
            config["NUM_EVAL_EPISODES"] * jnp.prod(jnp.array(batch_dims))
            / config["GPU_ENV_CAPACITY"]
        ))
        
        def _flatten_and_split_trainstate(trainstate):
            """
            Flatten training states across batch dimensions and split for sequential evaluation.
            
            This operation is JIT compiled for memory efficiency during evaluation.
            """
            flat_trainstate = jax.tree.map(
                lambda x: x.reshape((x.shape[0] * x.shape[1], *x.shape[2:])),
                trainstate
            )
            return _tree_split(flat_trainstate, n_sequential_evals)
        
        split_trainstate = jax.jit(_flatten_and_split_trainstate)(all_train_states)

        # Create evaluation environments for both train and test partner sets
        eval_train_env, run_eval_train = make_evaluation(config, load_zoo=load_zoo_dict_train)
        eval_test_env, run_eval_test = make_evaluation(config, load_zoo=load_zoo_dict_test)
        
        # Configure evaluation logging
        eval_log_config = EvalInfoLogConfig(
            env_state=False,
            done=True,
            action=False,
            value=False,
            reward=True,
            log_prob=False,
            obs=False,
            info=False,
            avail_actions=False,
        )

        # ===== DUAL EVALUATION EXECUTION =====
        print("Running dual evaluation (train vs test partners)...")
        
        # JIT compile evaluation functions for efficiency
        eval_train_jit = jax.jit(run_eval_train, static_argnames=["log_eval_info"])
        eval_train_vmap = jax.vmap(eval_train_jit, in_axes=(None, 0, None))
        eval_test_jit = jax.jit(run_eval_test, static_argnames=["log_eval_info"])
        eval_test_vmap = jax.vmap(eval_test_jit, in_axes=(None, 0, None))
        
        # Evaluate against training partners (seen during training)
        print("Evaluating against training partners...")
        evals_train = _concat_tree([
            eval_train_vmap(eval_rng, ts, eval_log_config)
            for ts in tqdm(split_trainstate, desc="Train partner evaluation")
        ])
        evals_train = jax.tree.map(
            lambda x: x.reshape((*batch_dims, *x.shape[1:])),
            evals_train
        )
        
        # Evaluate against test partners (unseen during training - zero-shot generalization)
        print("Evaluating against test partners (zero-shot generalization)...")
        evals_test = _concat_tree([
            eval_test_vmap(eval_rng, ts, eval_log_config)
            for ts in tqdm(split_trainstate, desc="Test partner evaluation")
        ])
        evals_test = jax.tree.map(
            lambda x: x.reshape((*batch_dims, *x.shape[1:])),
            evals_test
        )

        # ===== PERFORMANCE ANALYSIS =====
        print("Computing performance metrics...")
        
        # Compute returns for both train and test evaluations
        train_first_episode_returns = _compute_episode_returns(evals_train)
        train_first_episode_returns = train_first_episode_returns["__all__"]
        train_mean_episode_returns = train_first_episode_returns.mean(axis=-1)
        
        test_first_episode_returns = _compute_episode_returns(evals_test)
        test_first_episode_returns = test_first_episode_returns["__all__"]
        test_mean_episode_returns = test_first_episode_returns.mean(axis=-1)

        # Save evaluation results
        jnp.save("train_returns.npy", train_mean_episode_returns)
        jnp.save("test_returns.npy", test_mean_episode_returns)

        # ===== GENERALIZATION ANALYSIS =====
        print("\nAd Hoc Teamwork Results Summary:")
        print(f"Performance with training partners: {train_mean_episode_returns.mean():.2f} ± {train_mean_episode_returns.std():.2f}")
        print(f"Performance with test partners: {test_mean_episode_returns.mean():.2f} ± {test_mean_episode_returns.std():.2f}")
        



if __name__ == "__main__":
    main()


