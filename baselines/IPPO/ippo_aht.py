"""
IPPO Ad Hoc Teamwork (AHT) Training for Zero-Shot Generalization

This module implements Ad Hoc Teamwork training using IPPO, where agents learn to collaborate
effectively with diverse, pre-trained teammates from a zoo. The goal is to train agents that
can generalize to work with previously unseen partners in zero-shot scenarios.

Training Methodology:
1. Load diverse pre-trained partners from zoo (multiple algorithms)
2. Split partners into train/test sets for proper evaluation
3. Train new agents against the training set of partners
4. Evaluate on both train and test sets to measure generalization
5. Compare performance to assess zero-shot transfer capabilities

Usage:
    python ippo_aht.py [hydra options]
    
The script requires a pre-populated zoo (generated by ippo_zoo_gen.py) containing
diverse agents from multiple algorithms.
"""

import os
import time
from tqdm import tqdm
import jax
import jax.numpy as jnp
import flax.linen as nn
from flax.linen.initializers import constant, orthogonal
from flax.training.train_state import TrainState
from flax.traverse_util import flatten_dict
import safetensors.flax
import optax
import distrax
import jaxmarl
from jaxmarl.wrappers.baselines import get_space_dim, LogEnvState
from jaxmarl.wrappers.baselines import LogWrapper
from jaxmarl.wrappers.aht_all import ZooManager, LoadAgentWrapper
import hydra
from omegaconf import OmegaConf
import pandas as pd
from typing import Sequence, NamedTuple, Any, Dict


# ================================ TREE MANIPULATION UTILITIES ================================

def _tree_take(pytree, indices, axis=None):
    """
    Take elements from each leaf of a pytree along a specified axis.
    
    Essential for extracting specific training states during evaluation
    and partner selection from zoo populations.
    
    Args:
        pytree: JAX pytree (nested structure of arrays)
        indices: Indices to take from each array
        axis: Axis along which to take indices (None for flat indexing)
        
    Returns:
        Pytree with same structure but indexed arrays
    """
    return jax.tree.map(lambda x: x.take(indices, axis=axis), pytree)


def _tree_shape(pytree):
    """
    Get the shape of each leaf in a pytree.
    
    Useful for understanding batch dimensions when handling
    multiple partner configurations and training states.
    
    Args:
        pytree: JAX pytree (nested structure of arrays)
        
    Returns:
        Pytree with same structure but shapes instead of arrays
    """
    return jax.tree.map(lambda x: x.shape, pytree)


def _unstack_tree(pytree):
    """
    Unstack a pytree along the first axis, yielding a list of pytrees.
    
    Converts a pytree where each leaf has shape (N, ...) into a list of N pytrees
    where each leaf has shape (...). Useful for separating different seeds or agents.
    
    Args:
        pytree: JAX pytree with arrays of shape (N, ...)
        
    Returns:
        List of N pytrees, each with arrays of shape (...)
    """
    leaves, treedef = jax.tree_util.tree_flatten(pytree)
    unstacked_leaves = zip(*leaves)
    return [jax.tree_util.tree_unflatten(treedef, leaves)
            for leaves in unstacked_leaves]


def _stack_tree(pytree_list, axis=0):
    """
    Stack a list of pytrees along a specified axis.
    
    Args:
        pytree_list: List of pytrees with compatible structures
        axis: Axis along which to stack
        
    Returns:
        Single pytree with stacked arrays
    """
    return jax.tree.map(
        lambda *leaf: jnp.stack(leaf, axis=axis),
        *pytree_list
    )


def _concat_tree(pytree_list, axis=0):
    """
    Concatenate a list of pytrees along a specified axis.
    
    Args:
        pytree_list: List of pytrees with compatible structures
        axis: Axis along which to concatenate
        
    Returns:
        Single pytree with concatenated arrays
    """
    return jax.tree.map(
        lambda *leaf: jnp.concat(leaf, axis=axis),
        *pytree_list
    )


def _tree_split(pytree, n, axis=0):
    """
    Split a pytree into n parts along a specified axis.
    
    Args:
        pytree: JAX pytree to split
        n: Number of parts to split into
        axis: Axis along which to split
        
    Returns:
        List of n pytrees
    """
    leaves, treedef = jax.tree.flatten(pytree)
    split_leaves = zip(
        *jax.tree.map(lambda x: jnp.array_split(x, n, axis), leaves)
    )
    return [
        jax.tree.unflatten(treedef, leaves)
        for leaves in split_leaves
    ]


# ================================ EPISODE PROCESSING UTILITIES ================================

def _take_episode(pipeline_states, dones, time_idx=-1, eval_idx=0):
    """
    Extract a complete episode from evaluation data.
    
    Takes the pipeline states for a specific evaluation run and returns only
    the timesteps before the episode ended (excluding done states).
    
    Args:
        pipeline_states: Environment pipeline states for all timesteps
        dones: Boolean array indicating episode termination
        time_idx: Time axis index (default: -1)
        eval_idx: Which evaluation episode to extract (default: 0)
        
    Returns:
        List of pipeline states for the complete episode
    """
    episodes = _tree_take(pipeline_states, eval_idx, axis=1)
    dones = dones.take(eval_idx, axis=1)
    return [
        state
        for state, done in zip(_unstack_tree(episodes), dones)
        if not (done)
    ]


def _compute_episode_returns(eval_info, time_axis=-2):
    """
    Compute undiscounted episode returns from evaluation information.
    
    Handles episode boundaries correctly by resetting cumulative rewards
    when episodes end and start new ones.
    
    Args:
        eval_info: Evaluation information containing rewards and done flags
        time_axis: Axis representing time dimension (default: -2)
        
    Returns:
        Undiscounted returns for each episode
    """
    done_arr = eval_info.done["__all__"]
    
    # Create mask for episode boundaries
    first_timestep = [slice(None) for _ in range(done_arr.ndim)]
    first_timestep[time_axis] = 0
    episode_done = jnp.cumsum(done_arr, axis=time_axis, dtype=bool)
    episode_done = jnp.roll(episode_done, 1, axis=time_axis)
    episode_done = episode_done.at[tuple(first_timestep)].set(False)
    
    # Sum rewards within episodes only
    undiscounted_returns = jax.tree.map(
        lambda r: (r * (1 - episode_done)).sum(axis=time_axis),
        eval_info.reward
    )
    return undiscounted_returns


# ================================ MAIN AHT TRAINING FUNCTION ================================

@hydra.main(version_base=None, config_path="config", config_name="ippo_mabrax_aht")
def main(config):
    """
    Main function for Ad Hoc Teamwork training using IPPO with zoo-based partners.
    
    This function implements a sophisticated training pipeline for zero-shot generalization:
    1. Loads diverse pre-trained partners from zoo across multiple algorithms
    2. Creates train/test splits for proper generalization evaluation
    3. Trains new agents against the training set of diverse partners
    4. Evaluates on both train and test sets to measure generalization capability
    5. Saves comprehensive results for analysis of AHT performance
    
    The resulting agents should be able to collaborate effectively with previously
    unseen teammates, demonstrating robust teamwork and adaptability.
    
    Args:
        config: Hydra configuration object containing AHT training parameters,
               partner algorithms, and zoo specifications
    """
    config = OmegaConf.to_container(config, resolve=True)

    # ===== DYNAMIC ALGORITHM SELECTION =====
    # Import the appropriate IPPO variant based on network architecture configuration
    match (config["network"]["recurrent"], config["network"]["agent_param_sharing"]):
        case (False, False):
            from ippo_ff_nps import make_train, make_evaluation, EvalInfoLogConfig
            print("Using: Feedforward Networks with No Parameter Sharing")
        case (False, True):
            from ippo_ff_ps import make_train, make_evaluation, EvalInfoLogConfig
            print("Using: Feedforward Networks with Parameter Sharing")
        case (True, False):
            from ippo_rnn_nps import make_train, make_evaluation, EvalInfoLogConfig
            print("Using: Recurrent Networks with No Parameter Sharing")
        case (True, True):
            from ippo_rnn_ps import make_train, make_evaluation, EvalInfoLogConfig
            print("Using: Recurrent Networks with Parameter Sharing")

    # ===== TRAINING SETUP =====
    rng = jax.random.PRNGKey(config["SEED"])
    train_rng, eval_rng = jax.random.split(rng)
    train_rngs = jax.random.split(train_rng, config["NUM_SEEDS"])
    
    print(f"Starting Ad Hoc Teamwork training with {config['TOTAL_TIMESTEPS']} timesteps")
    print(f"Num environments: {config['NUM_ENVS']}")
    print(f"Num seeds: {config['NUM_SEEDS']}")
    print(f"Environment: {config['ENV_NAME']}")
    print(f"Zoo path: {config['ZOO_PATH']}")
    print(f"Training algorithm: {config['ALGORITHM']}")
    print(f"Partner algorithms: {config['PARTNER_ALGORITHMS']}")
    
    # ===== ZOO MANAGEMENT AND PARTNER SELECTION =====
    print("Loading zoo and selecting diverse partners...")
    with jax.disable_jit(config["DISABLE_JIT"]):
        zoo = ZooManager(config["ZOO_PATH"])
        alg = config["ALGORITHM"]
        scenario = config["ENV_NAME"]
        
        # Load partners from multiple algorithms for diversity
        partner_dict = {}
        for partner_algo in config["PARTNER_ALGORITHMS"]:
            partner_dict[partner_algo] = zoo.index.query(f'algorithm == "{partner_algo}"'
                                                         ).query(f'scenario == "{scenario}"'
                                                            ).query('scenario_agent_id == "human"')
            print(f"Found {len(partner_dict[partner_algo])} {partner_algo} partners")
        
        # ===== TRAIN/TEST SPLIT FOR GENERALIZATION EVALUATION =====
        # Critical for measuring zero-shot generalization capability
        print("Creating train/test splits for generalization evaluation...")
        train_set = {}
        test_set = {}
        for algo in partner_dict.keys():
            # 50/50 split ensures robust evaluation of generalization
            train_set[algo] = partner_dict[algo].sample(frac=0.5)
            test_set[algo] = partner_dict[algo].drop(train_set[algo].index)
            print(f"  {algo}: {len(train_set[algo])} train, {len(test_set[algo])} test partners")

        # Create zoo loading dictionaries for training and testing
        load_zoo_dict_train = {algo: {"human": list(train_set[algo].agent_uuid)} for algo in partner_dict.keys()}
        load_zoo_dict_test = {algo: {"human": list(test_set[algo].agent_uuid)} for algo in partner_dict.keys()}

        print(f"Training against {sum(len(train_set[algo]) for algo in train_set)} diverse partners")
        print(f"Testing against {sum(len(test_set[algo]) for algo in test_set)} unseen partners")

        # ===== AHT TRAINING EXECUTION =====
        print("Starting Ad Hoc Teamwork training...")
        env = jaxmarl.make(config["ENV_NAME"], **config["ENV_KWARGS"])
        
        # Create training function with zoo partner loading
        train_jit = jax.jit(
            make_train(
                config,
                save_train_state=True,
                load_zoo=load_zoo_dict_train  # Train against diverse partner set
            ),
            device=jax.devices()[config["DEVICE"]]
        )
        
        # Execute AHT training
        out = jax.vmap(train_jit, in_axes=(0, None, None, None))(
            train_rngs,
            config["LR"], config["ENT_COEF"], config["CLIP_EPS"]
        )

        # ===== SAVE TRAINING RESULTS =====
        print("Saving training results...")
        
        # Save training metrics (excluding large training states)
        EXCLUDED_METRICS = ["train_state"]
        jnp.save("metrics.npy", {
            key: val
            for key, val in out["metrics"].items()
            if key not in EXCLUDED_METRICS
            },
            allow_pickle=True
        )

        # Save model parameters
        all_train_states = out["metrics"]["train_state"]
        final_train_state = out["runner_state"].train_state
        
        safetensors.flax.save_file(
            flatten_dict(all_train_states.params, sep='/'),
            "all_params.safetensors"
        )
        
        if config["network"]["agent_param_sharing"]:
            # For parameter sharing: single set of shared parameters
            safetensors.flax.save_file(
                flatten_dict(final_train_state.params, sep='/'),
                "final_params.safetensors"
            )
        else:
            # For independent parameters: split by agent
            split_params = _unstack_tree(
                jax.tree.map(lambda x: x.swapaxes(0, 1), final_train_state.params)
            )
            for agent, params in zip(env.agents, split_params):
                safetensors.flax.save_file(
                    flatten_dict(params, sep='/'),
                    f"{agent}.safetensors",
                )

        # ===== GENERALIZATION EVALUATION SETUP =====
        print("Setting up generalization evaluation...")
        
        # Calculate evaluation batching for memory efficiency
        batch_dims = jax.tree.leaves(_tree_shape(all_train_states.params))[:2]
        n_sequential_evals = int(jnp.ceil(
            config["NUM_EVAL_EPISODES"] * jnp.prod(jnp.array(batch_dims))
            / config["GPU_ENV_CAPACITY"]
        ))
        
        def _flatten_and_split_trainstate(trainstate):
            """
            Flatten training states across batch dimensions and split for sequential evaluation.
            
            This operation is JIT compiled for memory efficiency during evaluation.
            """
            flat_trainstate = jax.tree.map(
                lambda x: x.reshape((x.shape[0] * x.shape[1], *x.shape[2:])),
                trainstate
            )
            return _tree_split(flat_trainstate, n_sequential_evals)
        
        split_trainstate = jax.jit(_flatten_and_split_trainstate)(all_train_states)

        # Create evaluation environments for both train and test partner sets
        eval_train_env, run_eval_train = make_evaluation(config, load_zoo=load_zoo_dict_train)
        eval_test_env, run_eval_test = make_evaluation(config, load_zoo=load_zoo_dict_test)
        
        # Configure evaluation logging
        eval_log_config = EvalInfoLogConfig(
            env_state=False,
            done=True,
            action=False,
            value=False,
            reward=True,
            log_prob=False,
            obs=False,
            info=False,
            avail_actions=False,
        )

        # ===== DUAL EVALUATION EXECUTION =====
        print("Running dual evaluation (train vs test partners)...")
        
        # JIT compile evaluation functions for efficiency
        eval_train_jit = jax.jit(run_eval_train, static_argnames=["log_eval_info"])
        eval_train_vmap = jax.vmap(eval_train_jit, in_axes=(None, 0, None))
        eval_test_jit = jax.jit(run_eval_test, static_argnames=["log_eval_info"])
        eval_test_vmap = jax.vmap(eval_test_jit, in_axes=(None, 0, None))
        
        # Evaluate against training partners (seen during training)
        print("Evaluating against training partners...")
        evals_train = _concat_tree([
            eval_train_vmap(eval_rng, ts, eval_log_config)
            for ts in tqdm(split_trainstate, desc="Train partner evaluation")
        ])
        evals_train = jax.tree.map(
            lambda x: x.reshape((*batch_dims, *x.shape[1:])),
            evals_train
        )
        
        # Evaluate against test partners (unseen during training - zero-shot generalization)
        print("Evaluating against test partners (zero-shot generalization)...")
        evals_test = _concat_tree([
            eval_test_vmap(eval_rng, ts, eval_log_config)
            for ts in tqdm(split_trainstate, desc="Test partner evaluation")
        ])
        evals_test = jax.tree.map(
            lambda x: x.reshape((*batch_dims, *x.shape[1:])),
            evals_test
        )

        # ===== PERFORMANCE ANALYSIS =====
        print("Computing performance metrics...")
        
        # Compute returns for both train and test evaluations
        train_first_episode_returns = _compute_episode_returns(evals_train)
        train_first_episode_returns = train_first_episode_returns["__all__"]
        train_mean_episode_returns = train_first_episode_returns.mean(axis=-1)
        
        test_first_episode_returns = _compute_episode_returns(evals_test)
        test_first_episode_returns = test_first_episode_returns["__all__"]
        test_mean_episode_returns = test_first_episode_returns.mean(axis=-1)

        # Save evaluation results
        jnp.save("train_returns.npy", train_mean_episode_returns)
        jnp.save("test_returns.npy", test_mean_episode_returns)

        # ===== GENERALIZATION ANALYSIS =====
        print("\nAd Hoc Teamwork Results Summary:")
        print(f"Performance with training partners: {train_mean_episode_returns.mean():.2f} ± {train_mean_episode_returns.std():.2f}")
        print(f"Performance with test partners: {test_mean_episode_returns.mean():.2f} ± {test_mean_episode_returns.std():.2f}")
        



if __name__ == "__main__":
    main()

# import os
# import time
# from tqdm import tqdm
# import jax
# import jax.numpy as jnp
# import flax.linen as nn
# from flax.linen.initializers import constant, orthogonal
# from flax.training.train_state import TrainState
# from flax.traverse_util import flatten_dict
# import safetensors.flax
# import optax
# import distrax
# import jaxmarl
# from jaxmarl.wrappers.baselines import get_space_dim, LogEnvState
# from jaxmarl.wrappers.baselines import LogWrapper
# from jaxmarl.wrappers.aht_all import ZooManager, LoadAgentWrapper
# import hydra
# from omegaconf import OmegaConf
# import pandas as pd
# from typing import Sequence, NamedTuple, Any, Dict



# def _tree_take(pytree, indices, axis=None):
#     return jax.tree.map(lambda x: x.take(indices, axis=axis), pytree)

# def _tree_shape(pytree):
#     return jax.tree.map(lambda x: x.shape, pytree)

# def _unstack_tree(pytree):
#     leaves, treedef = jax.tree_util.tree_flatten(pytree)
#     unstacked_leaves = zip(*leaves)
#     return [jax.tree_util.tree_unflatten(treedef, leaves)
#             for leaves in unstacked_leaves]

# def _stack_tree(pytree_list, axis=0):
#     return jax.tree.map(
#         lambda *leaf: jnp.stack(leaf, axis=axis),
#         *pytree_list
#     )

# def _concat_tree(pytree_list, axis=0):
#     return jax.tree.map(
#         lambda *leaf: jnp.concat(leaf, axis=axis),
#         *pytree_list
#     )

# def _tree_split(pytree, n, axis=0):
#     leaves, treedef = jax.tree.flatten(pytree)
#     split_leaves = zip(
#         *jax.tree.map(lambda x: jnp.array_split(x,n,axis), leaves)
#     )
#     return [
#         jax.tree.unflatten(treedef, leaves)
#         for leaves in split_leaves
#     ]

# def _take_episode(pipeline_states, dones, time_idx=-1, eval_idx=0):
#     episodes = _tree_take(pipeline_states, eval_idx, axis=1)
#     dones = dones.take(eval_idx, axis=1)
#     return [
#         state
#         for state, done in zip(_unstack_tree(episodes), dones)
#         if not (done)
#     ]

# def _compute_episode_returns(eval_info, time_axis=-2):
#     done_arr = eval_info.done["__all__"]
#     first_timestep = [slice(None) for _ in range(done_arr.ndim)]
#     first_timestep[time_axis] = 0
#     episode_done = jnp.cumsum(done_arr, axis=time_axis, dtype=bool)
#     episode_done = jnp.roll(episode_done, 1, axis=time_axis)
#     episode_done = episode_done.at[tuple(first_timestep)].set(False)
#     undiscounted_returns = jax.tree.map(
#         lambda r: (r*(1-episode_done)).sum(axis=time_axis),
#         eval_info.reward
#     )
#     return undiscounted_returns



# @hydra.main(version_base=None, config_path="config", config_name="ippo_mabrax_aht")
# def main(config):
#     config = OmegaConf.to_container(config, resolve=True)

#     # IMPORT FUNCTIONS BASED ON ARCHITECTURE
#     match (config["network"]["recurrent"], config["network"]["agent_param_sharing"]):
#         case (False, False):
#             from ippo_ff_nps_mabrax import make_train, make_evaluation, EvalInfoLogConfig
#         case (False, True):
#             from baselines.IPPO.ippo_ff_ps import make_train, make_evaluation, EvalInfoLogConfig
#         case (True, False):
#             from baselines.IPPO.ippo_rnn_nps import make_train, make_evaluation, EvalInfoLogConfig
#         case (True, True):
#             from baselines.IPPO.ippo_rnn_ps import make_train, make_evaluation, EvalInfoLogConfig

#     rng = jax.random.PRNGKey(config["SEED"])
#     train_rng, eval_rng = jax.random.split(rng)
#     train_rngs = jax.random.split(train_rng, config["NUM_SEEDS"])    
#     print(f"Starting training with {config['TOTAL_TIMESTEPS']} timesteps \n num envs: {config['NUM_ENVS']} \n num seeds: {config['NUM_SEEDS']} \n for env: {config['ENV_NAME']}")
#     with jax.disable_jit(config["DISABLE_JIT"]):
#         zoo = ZooManager(config["ZOO_PATH"])
#         alg = config["ALGORITHM"]
#         scenario = config["ENV_NAME"]
#         # index_filtered = zoo.index.query(f'scenario == "{scenario}"'
#         #                          ).query('scenario_agent_id == "human"')
        
#         partner_dict = {}
#         for partner_algo in config["PARTNER_ALGORITHMS"]:
#             partner_dict[partner_algo] = zoo.index.query(f'algorithm == "{partner_algo}"'
#                                                          ).query(f'scenario == "{scenario}"'
#                                                             ).query('scenario_agent_id == "human"')                                                        
            
#         train_set = {}
#         test_set = {}
#         for algo in partner_dict.keys():
#             train_set[algo] = partner_dict[algo].sample(frac=0.5)
#             test_set[algo] = partner_dict[algo].drop(train_set[algo].index)

#         load_zoo_dict_train = {algo: {"human": list(train_set[algo].agent_uuid)} for algo in partner_dict.keys()}
#         load_zoo_dict_test = {algo: {"human": list(test_set[algo].agent_uuid)} for algo in partner_dict.keys()}

#         # These are now shape {algo: {agent: {param_name: param_value}}}

#         env = jaxmarl.make(config["ENV_NAME"], **config["ENV_KWARGS"]) # This seems unnecessary
#         train_jit = jax.jit(
#             make_train(
#                 config,
#                 save_train_state=True,
#                 load_zoo=load_zoo_dict_train
#             ),
#             device=jax.devices()[config["DEVICE"]]
#         )
#         out = jax.vmap(train_jit, in_axes=(0, None, None, None))(
#             train_rngs,
#             config["LR"], config["ENT_COEF"], config["CLIP_EPS"]
#         )

#         # SAVE TRAIN METRICS
#         EXCLUDED_METRICS = ["train_state"]
#         jnp.save("metrics.npy", {
#             key: val
#             for key, val in out["metrics"].items()
#             if key not in EXCLUDED_METRICS
#             },
#             allow_pickle=True
#         )

#         # SAVE PARAMS
#         env = jaxmarl.make(config["ENV_NAME"], **config["ENV_KWARGS"])
#         all_train_states = out["metrics"]["train_state"]
#         final_train_state = out["runner_state"].train_state
#         safetensors.flax.save_file(
#             flatten_dict(all_train_states.params, sep='/'),
#             "all_params.safetensors"
#         )
#         if config["network"]["agent_param_sharing"]:
#             safetensors.flax.save_file(
#                 flatten_dict(final_train_state.params, sep='/'),
#                 "final_params.safetensors"
#             )
#         else:
#             # split by agent
#             split_params = _unstack_tree(
#                 jax.tree.map(lambda x: x.swapaxes(0,1), final_train_state.params)
#             )
#             for agent, params in zip(env.agents, split_params):
#                 safetensors.flax.save_file(
#                     flatten_dict(params, sep='/'),
#                     f"{agent}.safetensors",
#                 )

#         # RUN EVALUATION
#         # Assume the first 2 dimensions are batch dims
#         batch_dims = jax.tree.leaves(_tree_shape(all_train_states.params))[:2]
#         n_sequential_evals = int(jnp.ceil(
#             config["NUM_EVAL_EPISODES"] * jnp.prod(jnp.array(batch_dims))
#             / config["GPU_ENV_CAPACITY"]
#         ))
#         def _flatten_and_split_trainstate(trainstate):
#             # We define this operation and JIT it for memory reasons
#             flat_trainstate = jax.tree.map(
#                 lambda x: x.reshape((x.shape[0]*x.shape[1],*x.shape[2:])),
#                 trainstate
#             )
#             return _tree_split(flat_trainstate, n_sequential_evals)
#         split_trainstate = jax.jit(_flatten_and_split_trainstate)(all_train_states)

#         eval_train_env, run_eval_train = make_evaluation(config, load_zoo=load_zoo_dict_train)
#         eval_test_env, run_eval_test = make_evaluation(config, load_zoo=load_zoo_dict_test)
                
#         eval_log_config = EvalInfoLogConfig(
#             env_state=False,
#             done=True,
#             action=False,
#             value=False,
#             reward=True,
#             log_prob=False,
#             obs=False,
#             info=False,
#             avail_actions=False,
#         )
#         eval_train_jit = jax.jit(run_eval_train, static_argnames=["log_eval_info"])
#         eval_train_vmap = jax.vmap(eval_train_jit, in_axes=(None, 0, None))
#         eval_test_jit = jax.jit(run_eval_test, static_argnames=["log_eval_info"])
#         eval_test_vmap = jax.vmap(eval_test_jit, in_axes=(None, 0, None))
#         evals_train = _concat_tree([
#             eval_train_vmap(eval_rng, ts, eval_log_config)
#             for ts in tqdm(split_trainstate, desc="Evaluation batches")
#         ])
#         evals_train = jax.tree.map(
#             lambda x: x.reshape((*batch_dims, *x.shape[1:])),
#             evals_train
#         )
#         evals_test = _concat_tree([
#             eval_test_vmap(eval_rng, ts, eval_log_config)
#             for ts in tqdm(split_trainstate, desc="Evaluation batches")
#         ])
#         evals_test = jax.tree.map(
#             lambda x: x.reshape((*batch_dims, *x.shape[1:])),
#             evals_test
#         )

#         # COMPUTE RETURNS
#         train_first_episode_returns = _compute_episode_returns(evals_train)
#         train_first_episode_returns = train_first_episode_returns["__all__"]
#         train_mean_episode_returns = train_first_episode_returns.mean(axis=-1)
#         test_first_episode_returns = _compute_episode_returns(evals_test)
#         test_first_episode_returns = test_first_episode_returns["__all__"]
#         test_mean_episode_returns = test_first_episode_returns.mean(axis=-1)

#         # SAVE RETURNS
#         jnp.save("train_returns.npy", train_mean_episode_returns)
#         jnp.save("test_returns.npy", test_mean_episode_returns)



# if __name__ == "__main__":
#     main()
